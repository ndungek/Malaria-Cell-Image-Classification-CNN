{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DETECTING MALARIA USING CONVOLUTIONAL NEURAL NETWORKS: A TENSORFLOW APPROACH**\n",
        "\n",
        "`*Author:Maureen Kitang'a*`\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Malaria remains a significant public health concern worldwide, particularly in regions with limited access to healthcare resources. Rapid and accurate diagnosis of malaria is crucial for effective treatment and disease management. In recent years, deep learning techniques, particularly Convolutional Neural Networks (CNNs), have shown promising results in medical image analysis tasks, including malaria detection. In this project, we leverage the power of CNNs implemented using TensorFlow to create a robust model for malaria detection."
      ],
      "metadata": {
        "id": "xFXv2dj-F5KT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T2NJwcn8hLw"
      },
      "source": [
        "### *IMPORTING LIBRARIES*\n",
        "\n",
        "I began by importing numpy, pandas, and matplotlib. I decided to use `Keras` with `Tensorflow` backend to implement the CNN model. So, I imported a number of layers from keras.layers including `Convolution2D`, `MaxPooling2D`, `Flatten`, `Dense`, `BatchNormalization`, and `Dropout`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwR1O3Z08hL0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense,BatchNormalization,InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2F-4m6m8hL2"
      },
      "source": [
        "### *Getting the Data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0Y25QTg8hL2"
      },
      "outputs": [],
      "source": [
        "dataset, dataset_info = tfds.load('malaria',\n",
        "                                  with_info = True,\n",
        "                                  as_supervised= True,\n",
        "                                  shuffle_files= True,\n",
        "                                  split=['train'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to split the dataset into training, validation, and test sets\n",
        "def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n",
        "  DATASET_SIZE = len(dataset)\n",
        "  train_dataset = dataset.take(int(TRAIN_RATIO* DATASET_SIZE))\n",
        "\n",
        "  val_test_dataset = dataset.skip(int(TRAIN_RATIO* DATASET_SIZE))\n",
        "  val_dataset = val_test_dataset.take(int(VAL_RATIO * DATASET_SIZE))\n",
        "\n",
        "  test_dataset = val_test_dataset.skip(int(VAL_RATIO * DATASET_SIZE))\n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "Y0uN3FqWAIsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_BSES098hL3"
      },
      "outputs": [],
      "source": [
        "# Define ratios for train, validation, and test splits\n",
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n",
        "print(list(train_dataset.take(1).as_numpy_iterator()),\n",
        "      list(val_dataset.take(1).as_numpy_iterator()),list(test_dataset.take(1).as_numpy_iterator()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *DATA VISUALIZATION*"
      ],
      "metadata": {
        "id": "rW5fqYQo_2_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a sample of the dataset\n",
        "for i, (image, label) in enumerate(train_dataset.take(16)):\n",
        "  ax = plt.subplot(4, 4, i + 1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(dataset_info.features['label'].int2str(label))\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "Ca2_Hlza_xor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *DATA PREPROCESSING*"
      ],
      "metadata": {
        "id": "N007SpBgAQt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the images: resize and rescale\n",
        "IM_SIZE = 224\n",
        "def resize_rescale(image, label):\n",
        "  return tf.image.resize(image, (IM_SIZE, IM_SIZE))/255.0, label"
      ],
      "metadata": {
        "id": "Mhqkd8AvAKTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to train, validation, and test datasets\n",
        "train_dataset =  train_dataset.map(resize_rescale)\n",
        "val_dataset =  val_dataset.map(resize_rescale)\n",
        "test_dataset = test_dataset.map(resize_rescale)\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "YFDgYB49AXR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "6xAQPByWB4DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify preprocessing\n",
        "for data in train_dataset.take(1):\n",
        "  print(image, label)"
      ],
      "metadata": {
        "id": "Ldc6WQyYAacK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle and batch the datasets\n",
        "train_dataset = train_dataset.shuffle(buffer_size=8, reshuffle_each_iteration=True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.shuffle(buffer_size=8, reshuffle_each_iteration=True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "rbb-udeGAcZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print dataset information\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Validation Dataset:\", val_dataset)"
      ],
      "metadata": {
        "id": "MG_OB9dxAlGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *DATA MODELING*"
      ],
      "metadata": {
        "id": "yWWZbJOwCPp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n",
        "    Conv2D(filters=6, kernel_size=6, strides=1, padding='valid',activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = 2, strides = 2),\n",
        "\n",
        "    Conv2D(filters=16, kernel_size=3, strides=1, padding='valid',activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = 2, strides = 2),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(10, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "cnn_model.summary()\n",
        ""
      ],
      "metadata": {
        "id": "zGXpxFDAAvvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [0,1,0,0]\n",
        "y_pred = [0.6, 0.51, 0.94, 1]\n",
        "bce = tf.keras.losses.BinaryCrossentropy()\n",
        "bce(y_true, y_pred)"
      ],
      "metadata": {
        "id": "14TZMKxBCyRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.compile(optimizer = Adam(learning_rate=0.01),\n",
        "                    loss = BinaryCrossentropy(),\n",
        "                    metrics = 'accuracy')"
      ],
      "metadata": {
        "id": "NA9ZHpAwDM2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn_model.fit(train_dataset, validation_data=val_dataset, epochs = 20, verbose = 1)"
      ],
      "metadata": {
        "id": "6BMV_lLoDRZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xLLLwHw1Dl0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model_accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train_accuracy', 'val_accuracy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JEhj6D9NDrFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *MODEL EVALUATION AND TESTING*"
      ],
      "metadata": {
        "id": "1meSU2FvD6c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.batch(1)"
      ],
      "metadata": {
        "id": "EfJQv6DcD1o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "TvcPiiBdEE2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "OCpe3h2dEJ8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parasite_or_not(x):\n",
        "  if(x < 0.5):\n",
        "    return str('P')\n",
        "  else:\n",
        "    return str('U')"
      ],
      "metadata": {
        "id": "vVUdVDQiEV5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parasite_or_not(cnn_model.predict(test_dataset.take(1))[0][0])"
      ],
      "metadata": {
        "id": "6H1Mq-ejEKgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (image, label) in enumerate(test_dataset.take(9)):\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(image[0])\n",
        "  plt.title(str(parasite_or_not(label.numpy()[0])) + \":\" +str(parasite_or_not(cnn_model.predict(image)[0][0])))\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "BUtXnq0QEbZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.save(\"CNNSavedModel\")"
      ],
      "metadata": {
        "id": "5ICB4negFYhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.save(\"lenet.hdf5\")"
      ],
      "metadata": {
        "id": "dtnVzbllXdAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.save_weights(\"weights/cnn_weights\")"
      ],
      "metadata": {
        "id": "exgRosRVXdwT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}